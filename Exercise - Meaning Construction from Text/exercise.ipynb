{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.sparse\n",
    "import pandas as pd\n",
    "import sklearn.decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('brown')\n",
    "corpus = list(nltk.corpus.brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'and', 'to', 'a'] ['figured', 'Family', 'Abel', 'shaking', 'tent']\n"
     ]
    }
   ],
   "source": [
    "# Extract the 5000 most common English words W based on unigram frequencies.\n",
    "is_eng = lambda word: any(map(str.isalpha, word))\n",
    "eng_words = list(filter(is_eng, corpus))\n",
    "\n",
    "unigram_freq = nltk.FreqDist(nltk.ngrams(eng_words, 1))\n",
    "W = list(map(lambda x: x[0][0], unigram_freq.most_common(5000)))\n",
    "vocab = {word: idx for idx, word in enumerate(W)}\n",
    "\n",
    "# Report the 5 most and least common words in W. \n",
    "top_5, bottom_5 = W[:5], W[-5:]\n",
    "print(top_5, bottom_5)\n",
    "\n",
    "# Update W by words in Table 1 of RG65.\n",
    "df = pd.read_csv('./data/table1-rg65.csv', header=None)\n",
    "for word in pd.concat([df[0], df[1]]):\n",
    "    if word not in vocab:\n",
    "        W.append(word)\n",
    "        vocab[word] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct word-context vector model M1 by collecting bigram counts.\n",
    "data, row_ind, col_ind = [], [], []\n",
    "\n",
    "bigram_freq = nltk.FreqDist(nltk.ngrams(corpus, 2))\n",
    "for bigram, freq in bigram_freq.items():\n",
    "    if bigram[0] in vocab and bigram[1] in vocab:\n",
    "        data.append(freq)\n",
    "        row_ind.append(vocab[bigram[0]])\n",
    "        col_ind.append(vocab[bigram[1]])\n",
    "\n",
    "M1 = scipy.sparse.csr_array((data, (row_ind, col_ind)), shape=(len(vocab), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PPMI on M1 as M1+.\n",
    "total = M1.sum()\n",
    "word_cnt = M1.sum(axis=1)\n",
    "context_cnt = M1.sum(axis=0)\n",
    "\n",
    "joint = M1 / total\n",
    "marginal_word = word_cnt / total\n",
    "marginal_context = context_cnt / total\n",
    "\n",
    "# Avoid dividing by zero, will not affect the result.\n",
    "marginal_word[marginal_word == 0.] = 1.\n",
    "marginal_context[marginal_context == 0.] = 1.\n",
    "\n",
    "mi = joint \\\n",
    "     * np.expand_dims(np.reciprocal(marginal_word), axis=0).T \\\n",
    "     * np.reciprocal(marginal_context)\n",
    "\n",
    "pmi_data = np.log2(mi.data)\n",
    "ppmi_data = np.maximum(pmi_data, 0.)\n",
    "M1Plus = scipy.sparse.csr_array((ppmi_data, (mi.row, mi.col)), shape=mi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = M1.todense()\n",
    "M1Plus = M1Plus.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct latent semantic model M2 by applying PCA to M1+.\n",
    "pca_10 = sklearn.decomposition.PCA(n_components=10)\n",
    "pca_100 = sklearn.decomposition.PCA(n_components=100)\n",
    "pca_300 = sklearn.decomposition.PCA(n_components=300)\n",
    "\n",
    "M2_10 = pca_10.fit_transform(M1Plus)\n",
    "M2_100 = pca_100.fit_transform(M1Plus)\n",
    "M2_300 = pca_300.fit_transform(M1Plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation between Human and M1:\n",
      "PearsonRResult(statistic=0.34275259484379167, pvalue=0.005190853243447476)\n",
      "Pearson Correlation between Human and M1Plus:\n",
      "PearsonRResult(statistic=0.2572576769372493, pvalue=0.03856545520393161)\n",
      "Pearson Correlation between Human and M2_10:\n",
      "PearsonRResult(statistic=0.20332415786934097, pvalue=0.10427700948731429)\n",
      "Pearson Correlation between Human and M2_100:\n",
      "PearsonRResult(statistic=0.322681818672728, pvalue=0.008751880744247407)\n",
      "Pearson Correlation between Human and M2_300:\n",
      "PearsonRResult(statistic=0.29889190785068204, pvalue=0.015582299285943483)\n"
     ]
    }
   ],
   "source": [
    "# Find all pairs of words in Table 1 of RG65 that are also in W.\n",
    "# Record human-labeled similarities and calculate model-predicted similarities.\n",
    "def cosine_similarity(vec_1, vec_2):\n",
    "    norm_1 = np.linalg.norm(vec_1)\n",
    "    norm_2 = np.linalg.norm(vec_2)\n",
    "\n",
    "    if norm_1 == 0. or norm_2 == 0.:\n",
    "        return 0.\n",
    "\n",
    "    return np.dot(vec_1, vec_2) / (norm_1*norm_2)\n",
    "\n",
    "\n",
    "x, ys = [], [[], [], [], [], []]\n",
    "for _, (word_1, word_2, score) in df.iterrows():\n",
    "    if word_1 in vocab and word_2 in vocab:\n",
    "        x.append(score)\n",
    "\n",
    "        ind_1, ind_2 = vocab[word_1], vocab[word_2]\n",
    "        for idx, matrix in enumerate([M1, M1Plus, M2_10, M2_100, M2_300]):\n",
    "            ys[idx].append(cosine_similarity(matrix[ind_1], matrix[ind_2]))\n",
    "\n",
    "for y, matrix_name in zip(ys, ['M1', 'M1Plus', 'M2_10', 'M2_100', 'M2_300']):\n",
    "    r = scipy.stats.pearsonr(x, y)\n",
    "    print(f'Pearson Correlation between Human and {matrix_name}:\\n{r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d14af0e2168c83f18fd50138e0fb8fdefb5887620907a3a2caa0991ec5b1298"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
