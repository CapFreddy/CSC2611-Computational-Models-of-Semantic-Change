{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn.decomposition\n",
    "import scipy.stats\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('brown')\n",
    "all_words = list(map(str.lower, nltk.corpus.brown.words()))\n",
    "eng_words = list(filter(str.isalpha, all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'and', 'to', 'a'] ['vertex', 'rourke', 'killpath', 'haney', 'letch']\n",
      "['essay', 'distinguish', 'patents', 'therapist', 'damned', 'murderer', 'plantation', 'helion', 'rousseau', 'smelled', 'reactivity', 'tetrachloride', 'sera', 'nonspecific', 'vector', 'vertex', 'rourke', 'killpath', 'haney', 'letch']\n"
     ]
    }
   ],
   "source": [
    "# Extract the 5000 most frequent words W.\n",
    "unigrams = nltk.ngrams(eng_words, 1)\n",
    "unigrams_freq = nltk.FreqDist(unigrams)\n",
    "W = list(map(lambda x: x[0][0], unigrams_freq.most_common(5000)))\n",
    "vocab = {word: idx for idx, word in enumerate(W)}\n",
    "\n",
    "# Report the 5 most and least common words in W. \n",
    "top_5, bottom_5 = W[:5], W[-5:]\n",
    "print(top_5, bottom_5)\n",
    "print(W[-30:])\n",
    "\n",
    "# Update W by words in Table 1 of RG65.\n",
    "df = pd.read_csv('./synonymy.csv')\n",
    "for word in pd.concat([df.word_1, df.word_2]):\n",
    "    if word not in vocab and (word,) in unigrams_freq:\n",
    "        W.append(word)\n",
    "        vocab[word] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct word-context vector model M1.\n",
    "data, row_ind, col_ind = [], [], []\n",
    "\n",
    "bigrams = nltk.ngrams(all_words, 2)\n",
    "bigrams_freq = nltk.FreqDist(bigrams)\n",
    "for bigram, freq in bigrams_freq.items():\n",
    "    if bigram[0] in vocab and bigram[1] in vocab:\n",
    "        data.append(freq)\n",
    "        row_ind.append(vocab[bigram[0]])\n",
    "        col_ind.append(vocab[bigram[1]])\n",
    "\n",
    "M1 = scipy.sparse.csr_array((data, (row_ind, col_ind)), shape=(len(W), len(W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fred\\AppData\\Local\\Temp\\ipykernel_2448\\2299949177.py:11: RuntimeWarning: divide by zero encountered in reciprocal\n",
      "  * np.expand_dims(np.reciprocal(word_marginal), axis=0).T \\\n",
      "C:\\Users\\Fred\\AppData\\Local\\Temp\\ipykernel_2448\\2299949177.py:12: RuntimeWarning: divide by zero encountered in reciprocal\n",
      "  * np.reciprocal(context_marginal)\n"
     ]
    }
   ],
   "source": [
    "# Compute PPMI on M1 as M1+.\n",
    "tot_sum = M1.sum()\n",
    "word_sum = M1.sum(axis=1)\n",
    "context_sum = M1.sum(axis=0)\n",
    "\n",
    "joint = M1 / tot_sum\n",
    "word_marginal = word_sum / tot_sum\n",
    "context_marginal = context_sum / tot_sum\n",
    "\n",
    "mi = joint \\\n",
    "     * np.expand_dims(np.reciprocal(word_marginal), axis=0).T \\\n",
    "     * np.reciprocal(context_marginal)\n",
    "pmi_data = np.log2(mi.data)\n",
    "ppmi_data = np.maximum(pmi_data, 0.)\n",
    "M1Plus = scipy.sparse.csr_array((ppmi_data, (mi.row, mi.col)), shape=mi.shape).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct latent semantic model M2 by applying PCA to M1+.\n",
    "pca_10 = sklearn.decomposition.PCA(n_components=10)\n",
    "pca_100 = sklearn.decomposition.PCA(n_components=100)\n",
    "pca_300 = sklearn.decomposition.PCA(n_components=300)\n",
    "\n",
    "M2_10 = pca_10.fit_transform(M1Plus)\n",
    "M2_100 = pca_100.fit_transform(M1Plus)\n",
    "M2_300 = pca_300.fit_transform(M1Plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation between Human and M1:\n",
      "PearsonRResult(statistic=0.32055637738466136, pvalue=0.009810288765306442)\n",
      "Pearson Correlation between Human and M1Plus:\n",
      "PearsonRResult(statistic=0.2031487194875153, pvalue=0.10739872408309742)\n",
      "Pearson Correlation between Human and M2_10:\n",
      "PearsonRResult(statistic=0.1904962659203761, pvalue=0.13161122162653538)\n",
      "Pearson Correlation between Human and M2_100:\n",
      "PearsonRResult(statistic=0.3734250770679275, pvalue=0.0023713847821895395)\n",
      "Pearson Correlation between Human and M2_300:\n",
      "PearsonRResult(statistic=0.35605460741686434, pvalue=0.003882223705338614)\n"
     ]
    }
   ],
   "source": [
    "# Find all pairs of words in Table 1 of RG65 that are also in W.\n",
    "# Record human-labeled similarities and calculate model-predicted similarities.\n",
    "def cosine_similarity(vec_1, vec_2):\n",
    "    norm_1 = np.linalg.norm(vec_1)\n",
    "    norm_2 = np.linalg.norm(vec_2)\n",
    "\n",
    "    if norm_1 == 0. or norm_2 == 0.:\n",
    "        return 0.\n",
    "\n",
    "    return np.dot(vec_1, vec_2) / (norm_1*norm_2)\n",
    "\n",
    "\n",
    "x, ys = [], [[], [], [], [], []]\n",
    "for _, (word_1, word_2, score) in df.iterrows():\n",
    "    if word_1 in vocab and word_2 in vocab:\n",
    "        x.append(score)\n",
    "\n",
    "        ind_1, ind_2 = vocab[word_1], vocab[word_2]\n",
    "        for idx, matrix in enumerate([M1.todense(), M1Plus, M2_10, M2_100, M2_300]):\n",
    "        # for idx, matrix in enumerate([M1.todense()]):\n",
    "            ys[idx].append(cosine_similarity(matrix[ind_1], matrix[ind_2]))\n",
    "\n",
    "for y, matrix_name in zip(ys, ['M1', 'M1Plus', 'M2_10', 'M2_100', 'M2_300']):\n",
    "# for y, matrix_name in zip(ys, ['M1']):\n",
    "    r = scipy.stats.pearsonr(x, y)\n",
    "    print(f'Pearson Correlation between Human and {matrix_name}:\\n{r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d14af0e2168c83f18fd50138e0fb8fdefb5887620907a3a2caa0991ec5b1298"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
